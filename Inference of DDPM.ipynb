{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_0$ be the original image and $X_1, ..., X_T$ be the latent variables at different timesteps. We assume the transitions to be first order markovian chain:\n",
    "$$X_0 \\rightarrow X_1 \\rightarrow X_2 - - \\rightarrow X_T$$\n",
    "The transitions are gaussian:\n",
    "$$X_t = \\sqrt{\\alpha_t} X_{t-1} + \\sqrt{1-\\alpha_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use latent variables $X_t$ to model the data as:\n",
    "\n",
    "$$\\log p_\\theta(X_0) = \\log \\int p_\\theta(X_{0:T}) dX_{1:T}$$\n",
    "\n",
    "We also assume that: $p_\\theta(X_T) = \\mathcal{N}(0, I)$ - The prior distribution of the final latent variable is a standard normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we cannot compute log likelihood directly, we maximize the ELBO:\n",
    "$$\\log p_\\theta(X_0) \\geq \\mathbb{E}_{q(X_{1:T}|X_0)}[\\log \\frac{p_\\theta(X_{0:T})}{q(X_{1:T}|X_0)}]$$\n",
    "\n",
    "After simplifying, we get:\n",
    "$$ ELBO = \\mathbb{E}_{q(X_{1:T}|X_0)}\\left[\\log p_\\theta(X_T) + \\log \\left(\\frac{p(X_T|X_0)}{q(X_T|X_0)}\\right) + \\sum_{t=2}^T \\log \\left(\\frac{p_\\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_t,X_0)}\\right)\\right]$$\n",
    "\n",
    "- Term 1(Reconstruction term): $\\log p_\\theta(X_T)$ - ignored in the original DDPM paper because it only applies to the first timestep\n",
    "\n",
    "- Term 2(Prior matching term): $\\log \\left(\\frac{p(X_T|X_0)}{q(X_T|X_0)}\\right)$ - Can be ignored as it is invariant to $\\theta$\n",
    "\n",
    "- Term 3(Transition matching term): $\\sum_{t=2}^T \\log \\left(\\frac{p_\\theta(X_{t-1}|X_t)}{q(X_{t-1}|X_t,X_0)}\\right) \\propto \\sum_{t=2}^T \\mathbb{E}_{q(x_t|x_0)} \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|_2^2$ used to train the network to predict the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'  # Prevent OpenMP initialization error\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torch.optim as optim  # Import optimization module\n",
    "import math  # Import math module for log calculations\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms\n",
    "from torchvision.utils import save_image, make_grid  # Import utility to save images\n",
    "import torchvision  # Import torchvision library\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import os  # Import os module for file operations\n",
    "import numpy as np  # Import numpy library\n",
    "from torch.utils.data import Dataset  # Add this import at the top\n",
    "from PIL import Image  # Import PIL Image module for image handling\n",
    "import torch.nn.functional as F  # Import PyTorch's functional API for loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture for noise prediction in diffusion models with built-in residual connections, optimized for 128x128 RGB images\"\"\"\n",
    "    def __init__(self, in_channels=3, time_dim=256):  # Modified for RGB input (3 channels)\n",
    "        super().__init__()\n",
    "\n",
    "        # Pooling and activation layers used throughout the network\n",
    "        self.pool = nn.MaxPool2d(2)  # Max pooling for downsampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # Bilinear upsampling\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "\n",
    "        # Encoder Block 1 - Input level (128x128)\n",
    "        self.enc1_conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)  # First conv: (N,3,128,128) -> (N,64,128,128)\n",
    "        self.enc1_bn1 = nn.BatchNorm2d(64)  # Normalizes each of the 64 channels independently\n",
    "        self.enc1_conv2 = nn.Conv2d(64, 64, 3, padding=1)  # Second conv: (N,64,128,128) -> (N,64,128,128)\n",
    "        self.enc1_bn2 = nn.BatchNorm2d(64)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 2 - After first pooling (64x64)\n",
    "        self.enc2_conv1 = nn.Conv2d(64, 128, 3, padding=1)  # First conv: (N,64,64,64) -> (N,128,64,64)\n",
    "        self.enc2_bn1 = nn.BatchNorm2d(128)  # Batch norm after first conv\n",
    "        self.enc2_conv2 = nn.Conv2d(128, 128, 3, padding=1)  # Second conv: (N,128,64,64) -> (N,128,64,64)\n",
    "        self.enc2_bn2 = nn.BatchNorm2d(128)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 3 - After second pooling (32x32)\n",
    "        self.enc3_conv1 = nn.Conv2d(128, 256, 3, padding=1)  # First conv: (N,128,32,32) -> (N,256,32,32)\n",
    "        self.enc3_bn1 = nn.BatchNorm2d(256)  # Batch norm after first conv\n",
    "        self.enc3_conv2 = nn.Conv2d(256, 256, 3, padding=1)  # Second conv: (N,256,32,32) -> (N,256,32,32)\n",
    "        self.enc3_bn2 = nn.BatchNorm2d(256)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 3 - First upsampling (32x32 -> 64x64)\n",
    "        self.dec3_conv1 = nn.Conv2d(384, 128, 3, padding=1)  # First conv: (N,384,64,64) -> (N,128,64,64)\n",
    "        self.dec3_bn1 = nn.BatchNorm2d(128)  # Batch norm after first conv\n",
    "        self.dec3_conv2 = nn.Conv2d(128, 128, 3, padding=1)  # Second conv: (N,128,64,64) -> (N,128,64,64)\n",
    "        self.dec3_bn2 = nn.BatchNorm2d(128)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 2 - Second upsampling (64x64 -> 128x128)\n",
    "        self.dec2_conv1 = nn.Conv2d(192, 64, 3, padding=1)  # First conv: (N,192,128,128) -> (N,64,128,128)\n",
    "        self.dec2_bn1 = nn.BatchNorm2d(64)  # Batch norm after first conv\n",
    "        self.dec2_conv2 = nn.Conv2d(64, 64, 3, padding=1)  # Second conv: (N,64,128,128) -> (N,64,128,128)\n",
    "        self.dec2_bn2 = nn.BatchNorm2d(64)  # Batch norm after second conv\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(64, in_channels, kernel_size=1)  # Final conv: (N,64,128,128) -> (N,3,128,128)\n",
    "\n",
    "        # Time embedding dimension and projection\n",
    "        self.time_dim = time_dim  # Time embedding dimension\n",
    "\n",
    "        # Define MLPs as model parameters\n",
    "        self.time_enc1 = nn.Sequential(nn.Linear(time_dim, 64), nn.SiLU(), nn.Linear(64, 64))  # Time embedding MLP for encoder block 1\n",
    "        self.time_enc2 = nn.Sequential(nn.Linear(time_dim, 128), nn.SiLU(), nn.Linear(128, 128))  # Time embedding MLP for encoder block 2\n",
    "        self.time_enc3 = nn.Sequential(nn.Linear(time_dim, 256), nn.SiLU(), nn.Linear(256, 256))  # Time embedding MLP for encoder block 3\n",
    "        self.time_dec3 = nn.Sequential(nn.Linear(time_dim, 128), nn.SiLU(), nn.Linear(128, 128))  # Time embedding MLP for decoder block 3\n",
    "        self.time_dec2 = nn.Sequential(nn.Linear(time_dim, 64), nn.SiLU(), nn.Linear(64, 64))  # Time embedding MLP for decoder block 2\n",
    "\n",
    "    def get_time_embedding(self, t):\n",
    "        \"\"\"Generate sinusoidal time embedding and project through MLPs for each block\n",
    "\n",
    "        Args:\n",
    "            t: Time tensor of shape (batch_size, 1)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing time embeddings for each block\n",
    "        \"\"\"\n",
    "        half_dim = self.time_dim // 2  # Calculate half dimension for sin/cos embeddings\n",
    "        embeddings = torch.arange(half_dim, device=t.device).float()  # Create position indices\n",
    "        embeddings = torch.exp(-math.log(10000) * embeddings / half_dim)  # Calculate frequency bands\n",
    "        embeddings = t * embeddings.unsqueeze(0)  # Shape: (batch_size, half_dim)\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)  # Shape: (batch_size, time_dim)\n",
    "\n",
    "        # Use the class MLPs instead of creating new ones\n",
    "        t_emb_enc1 = self.time_enc1(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc2 = self.time_enc2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc3 = self.time_enc3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec3 = self.time_dec3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec2 = self.time_dec2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "\n",
    "        return {\n",
    "            'enc1': t_emb_enc1,  # Time embedding for encoder block 1\n",
    "            'enc2': t_emb_enc2,  # Time embedding for encoder block 2\n",
    "            'enc3': t_emb_enc3,  # Time embedding for encoder block 3\n",
    "            'dec3': t_emb_dec3,  # Time embedding for decoder block 3\n",
    "            'dec2': t_emb_dec2   # Time embedding for decoder block 2\n",
    "        }\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Forward pass through U-Net optimized for 128x128 RGB input with time embeddings at each block\"\"\"\n",
    "        # Time embedding\n",
    "        t = t.unsqueeze(-1).float()  # Ensure time is in correct shape\n",
    "        t_embs = self.get_time_embedding(t)  # Get time embeddings for each block\n",
    "\n",
    "        # Encoder pathway with skip connections and time embeddings\n",
    "        # Encoder Block 1 (128x128)\n",
    "        e1 = self.relu(self.enc1_bn1(self.enc1_conv1(x)))  # First conv layer\n",
    "        e1 = self.relu(self.enc1_bn2(self.enc1_conv2(e1)))  # Second conv layer with ReLU\n",
    "        e1 = e1 + t_embs['enc1']  # Add time embedding to encoder block 1\n",
    "\n",
    "        # Encoder Block 2 (64x64)\n",
    "        e2 = self.relu(self.enc2_bn1(self.enc2_conv1(self.pool(e1))))  # First conv layer\n",
    "        e2 = self.relu(self.enc2_bn2(self.enc2_conv2(e2)))  # Second conv layer with ReLU\n",
    "        e2 = e2 + t_embs['enc2']  # Add time embedding to encoder block 2\n",
    "\n",
    "        # Encoder Block 3 (32x32)\n",
    "        e3 = self.relu(self.enc3_bn1(self.enc3_conv1(self.pool(e2))))  # First conv layer\n",
    "        e3 = self.relu(self.enc3_bn2(self.enc3_conv2(e3)))  # Second conv layer with ReLU\n",
    "        e3 = e3 + t_embs['enc3']  # Add time embedding to encoder block 3\n",
    "\n",
    "        # Decoder pathway using skip connections\n",
    "        # Decoder Block 3 (32x32 -> 64x64)\n",
    "        d3 = torch.cat([self.upsample(e3), e2], dim=1)  # Concatenate along channel dimension\n",
    "        d3 = self.relu(self.dec3_bn1(self.dec3_conv1(d3)))  # First conv block\n",
    "        d3 = self.dec3_bn2(self.dec3_conv2(d3))  # Second conv block\n",
    "        d3 = d3 + t_embs['dec3']  # Add time embedding to decoder block 3\n",
    "\n",
    "        # Decoder Block 2 (64x64 -> 128x128)\n",
    "        d2 = torch.cat([self.upsample(d3), e1], dim=1)  # Concatenate along channel dimension\n",
    "        d2 = self.relu(self.dec2_bn1(self.dec2_conv1(d2)))  # First conv block\n",
    "        d2 = self.dec2_bn2(self.dec2_conv2(d2))  # Second conv block\n",
    "        d2 = d2 + t_embs['dec2']  # Add time embedding to decoder block 2\n",
    "\n",
    "        return self.final_conv(d2)  # Return final output (N,3,128,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward diffusion process gradually adds Gaussian noise to an image over multiple timesteps. At each timestep $t$, we add noise according to a schedule, transforming a clear image $x_0$ into increasingly noisy versions $x_1$, $x_2$, ..., $x_t$. The amount of noise added is controlled by the diffusion parameters $\\alpha$ and $\\beta$. The forward process equation is:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha_t}} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\cdot \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $x_t$ is the noisy image at timestep $t$\n",
    "- $x_0$ is the original clean image  \n",
    "- $\\bar{\\alpha_t}$ (alpha_bar) is the cumulative product of $\\alpha_i = (1-\\beta_i)$ up to timestep $t$, i.e. $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
    "- $\\epsilon$ (epsilon) is random Gaussian noise\n",
    "\n",
    "This process gradually transforms the data distribution into pure Gaussian noise at $t=T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BETA_START = 0.0001  # Start value for noise schedule\n",
    "BETA_END = 0.02  # End value for noise schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_at_timestep(x_start, t, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Add noise to images at timestep t according to diffusion process\n",
    "\n",
    "    Args:\n",
    "        x_start (torch.Tensor): Original images\n",
    "        t (torch.Tensor): Timesteps\n",
    "        timesteps (int): Total number of diffusion steps\n",
    "\n",
    "    Returns:\n",
    "        tuple: Noisy images and noise\n",
    "    \"\"\"\n",
    "    device = x_start.device  # Get device from input tensor\n",
    "    noise = torch.randn_like(x_start)  # Generate random noise on same device as x_start\n",
    "\n",
    "    betas = torch.linspace(BETA_START, BETA_END, timesteps).to(device)  # Move noise schedule to device\n",
    "    alphas = 1 - betas  # Alpha values\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)  # Cumulative product of alphas\n",
    "\n",
    "    # Extract relevant alpha values for timestep t\n",
    "    sqrt_alphas_cumprod_t = alphas_cumprod[t].sqrt()  # Get sqrt(alpha_bar) for timestep t\n",
    "    sqrt_one_minus_alphas_cumprod_t = (1 - alphas_cumprod[t]).sqrt()  # Get sqrt(1-alpha_bar) for timestep t\n",
    "\n",
    "    # Reshape for broadcasting\n",
    "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, 1, 1, 1)  # Shape: (batch_size, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, 1, 1, 1)  # Shape: (batch_size, 1, 1, 1)\n",
    "\n",
    "    # Apply noise using the diffusion equation\n",
    "    noisy_images = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise  # Forward diffusion step\n",
    "\n",
    "    return noisy_images, noise  # Return both noisy images and the noise added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its simply mean square error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model, x_start, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Calculate the diffusion loss across multiple timesteps for each image in batch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The UNet model for noise prediction\n",
    "        x_start (torch.Tensor): Original clean images (batch_size, channels, height, width)\n",
    "        timesteps (int): Total number of diffusion steps\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mean loss per image in batch\n",
    "    \"\"\"\n",
    "    batch_size = x_start.shape[0]\n",
    "\n",
    "    # Initialize total loss for each image\n",
    "    total_loss = torch.zeros(batch_size, device=x_start.device)\n",
    "\n",
    "    # Sample 10 timesteps for each image\n",
    "    for _ in range(10):\n",
    "        # Sample random timesteps for each image in the batch\n",
    "        t = torch.randint(1, timesteps, (batch_size,), device=x_start.device)\n",
    "\n",
    "        # Add noise to the input images for the sampled timesteps\n",
    "        noisy_images, noise = add_noise_at_timestep(x_start, t, timesteps)\n",
    "\n",
    "        # Predict the noise using the model\n",
    "        predicted_noise = model(noisy_images, t)\n",
    "\n",
    "        # Calculate MSE loss between predicted and actual noise per image\n",
    "        step_loss = F.mse_loss(predicted_noise, noise, reduction='none')  # Shape: (batch_size, channels, H, W)\n",
    "        step_loss = step_loss.mean(dim=(1,2,3))  # Average over channels, height, width\n",
    "\n",
    "        # Add to total loss\n",
    "        total_loss += step_loss\n",
    "\n",
    "    # Average loss across the 10 timesteps\n",
    "    avg_loss = total_loss / 10\n",
    "\n",
    "    return avg_loss  # Return average loss per image in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2  # Number of images to process in each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128 pixels\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor and scale to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir  # Directory containing all images\n",
    "        self.transform = transform  # Transformations to apply to images\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]  # List all image files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)  # Return the total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])  # Get path of image at index idx\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if any\n",
    "\n",
    "        return image, 0  # Return image and a dummy label (0)\n",
    "\n",
    "# Load the Butterfly dataset from local machine\n",
    "data_dir = r'D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset'  # Path to the Butterfly dataset\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomImageDataset(root_dir=data_dir, transform=transform)  # Use our custom dataset class\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)  # Create dataloader\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images.\")  # Print the total number of images loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and move to device\n",
    "model = UNet()  # Create UNet model instance\n",
    "model = model.to(device)  # Move model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode\n",
    "model.train()  # Enable training mode for model (activates dropout, batch norm, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "num_epochs = 100  # Number of epochs to train for\n",
    "learning_rate = 1e-4  # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Initialize Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop implementation\n",
    "for epoch in range(num_epochs):  # Iterate through epochs\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")  # Print current epoch progress\n",
    "\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):  # Iterate through batches\n",
    "        optimizer.zero_grad()  # Reset gradients for this batch\n",
    "\n",
    "        images = images.to(device)  # Move input images to device\n",
    "\n",
    "        # Calculate diffusion loss using the modified loss function\n",
    "        batch_losses = diffusion_loss_fn(model, images)  # Get per-image losses\n",
    "        loss = batch_losses.mean()  # Average loss across batch\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.4f}\")  # Print current batch loss and value\n",
    "\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Clear GPU cache every few batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save model checkpoint after each epoch\n",
    "    torch.save(model.state_dict(), f'unet_model_epoch_{epoch+1}.pth')  # Save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference process in DDPM involves reversing the diffusion process to generate new samples. The steps are as follows:\n",
    "\n",
    "1. Sample from the prior: Start by sampling $x_T \\sim \\mathcal{N}(0, I)$, which is the prior distribution.\n",
    "\n",
    "2. Reverse the diffusion process: Sequentially get $x_{t-1}$ from xt for $t = T, T-1, \\ldots, 1$ using\n",
    "   \n",
    "    $$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon(x_t, t)) + \\sigma_t z$$\n",
    "\n",
    "    where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t^2 = \\beta_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}(1-\\alpha_t)$\n",
    "\n",
    "3. Obtain the final sample: The final sample x0 is obtained after completing the reverse process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_schedule(num_timesteps):\n",
    "    \"\"\"\n",
    "    Get the beta schedule for the diffusion process\n",
    "\n",
    "    Args:\n",
    "        num_timesteps (int): Number of timesteps in diffusion process\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Beta schedule tensor\n",
    "    \"\"\"\n",
    "    # Create linear schedule from BETA_START to BETA_END\n",
    "    return torch.linspace(BETA_START, BETA_END, num_timesteps).to('cuda')  # Return beta schedule on GPU\n",
    "\n",
    "def generate_sample(model, num_timesteps, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate a new RGB image sample by reversing the diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: UNet model instance trained on diffusion process\n",
    "        num_timesteps: Number of timesteps in the diffusion process\n",
    "        device: Device to run generation on (default: 'cuda')\n",
    "\n",
    "    Returns:\n",
    "        x_0: Generated RGB image sample after complete reverse diffusion\n",
    "    \"\"\"\n",
    "    # Start with random noise from standard normal distribution\n",
    "    x_t = torch.randn(1, 3, 128, 128).to(device)\n",
    "\n",
    "    # Get pre-computed diffusion parameters\n",
    "    betas = get_beta_schedule(num_timesteps)\n",
    "    alphas = 1 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(num_timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "\n",
    "            predicted_noise = model(x_t, t_tensor)\n",
    "\n",
    "            # Calculate reverse process parameters\n",
    "            alpha_t = alphas[t]\n",
    "            alpha_t_bar = alphas_cumprod[t]\n",
    "\n",
    "            # Calculate the correct variance term\n",
    "            if t > 0:\n",
    "                alpha_t_bar_prev = alphas_cumprod[t-1]\n",
    "                sigma_t = torch.sqrt(\n",
    "                    ((1 - alpha_t_bar_prev) / (1 - alpha_t_bar)) * (1 - alpha_t)\n",
    "                )\n",
    "            else:\n",
    "                sigma_t = torch.zeros_like(alpha_t)\n",
    "\n",
    "            # Only add random noise if t > 0\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = 0\n",
    "\n",
    "            # Reverse diffusion step formula\n",
    "            x_t = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_t_bar)) * predicted_noise\n",
    "            ) + sigma_t * noise\n",
    "\n",
    "    x_0 = (torch.clamp(x_t, -1, 1) + 1) / 2\n",
    "\n",
    "    return x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and generate samples\n",
    "model_path = r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\unet_model_epoch_11.pth\"\n",
    "loaded_model = UNet().to('cuda')\n",
    "\n",
    "# Load the checkpoint dictionary\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the model state dict from the checkpoint\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "loaded_model.eval()\n",
    "num_timesteps = 1000  # Use same number of timesteps as training\n",
    "\n",
    "# Generate and save 100 images\n",
    "for i in range(100):\n",
    "    # Generate sample using the loaded model\n",
    "    generated_sample = generate_sample(loaded_model, num_timesteps, device='cuda')\n",
    "\n",
    "    # Convert tensor to image\n",
    "    generated_image = generated_sample[0].cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Save the image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(generated_image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, f'generated_image_{i+1}.png'),\n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Generated {i+1}/100 images\")\n",
    "\n",
    "print(\"All images generated and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForADRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
